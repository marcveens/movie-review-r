set.seed(1)
df <- df[sample(nrow(df)), ]
df <- df[sample(nrow(df)), ]
glimpse(df)
df$sentiment <- as.factor(df$sentiment)
corpus <- Corpus(VectorSource(df$text))
corpus
inspect(corpus[1:3])
corpus.clean <- corpus %>%
tm_map(content_transformer(tolower)) %>%
tm_map(removePunctuation) %>%
tm_map(removeNumbers) %>%
tm_map(removeWords, stopwords(kind="en")) %>%
tm_map(stripWhitespace)
dtm <- DocumentTermMatrix(corpus.clean)
df.train <- df[1:1500,]
df.test <- df[1501:2000,]
dtm.train <- dtm[1:1500,]
dtm.test <- dtm[1501:2000,]
corpus.clean.train <- corpus.clean[1:1500]
corpus.clean.test <- corpus.clean[1501:2000]
dim(dtm.train)
fivefreq <- findFreqTerms(dtm.train, 5)
length((fivefreq))
## [1] 12144
# Use only 5 most frequent words (fivefreq) to build the DTM
dtm.train.nb <- DocumentTermMatrix(corpus.clean.train, control=list(dictionary = fivefreq))
dim(dtm.train.nb)
## [1]  1500 12144
dtm.test.nb <- DocumentTermMatrix(corpus.clean.test, control=list(dictionary = fivefreq))
dim(dtm.train.nb)
convert_count <- function(x) {
y <- ifelse(x > 0, 1,0)
y <- factor(y, levels=c(0,1), labels=c("No", "Yes"))
y
}
trainNB <- apply(dtm.train.nb, 2, convert_count)
testNB <- apply(dtm.test.nb, 2, convert_count)
system.time( classifier <- naiveBayes(trainNB, df.train$sentiment, laplace = 1) )
system.time( pred <- predict(classifier, newdata=testNB) )
table("Predictions"= pred,  "Actual" = df.test$sentiment )
conf.mat <- confusionMatrix(pred, df.test$sentiment)
conf.mat
glimpse(df)
View(df)
inspect(corpus[1:3])
View(df.test)
## Dataset 1: https://www.kaggle.com/c/word2vec-nlp-tutorial
## Dataset 2: https://www.kaggle.com/nltkdata/movie-review
# install.packages("readtext")
# install.packages("dplyr")
# install.packages("mongolite")
library(readtext)
library(dplyr)
library(mongolite)
getValuesFromTxt <- function(files, sentiment) {
ds <- readtext::readtext(files)
ds$doc_id <- NULL;
ds$sentiment <- sentiment;
colnames(ds)[1] <- "text"
ds
}
setwd("C:\Users\MarcVe\OneDrive\Documenten\HvA\2017\Data analysis\Individual")
data <- read.csv2("dataset/1/labeledTrainData.csv", sep="\t")
data$id <- NULL
colnames(data)[2] <- "text"
dataset2neg <- getValuesFromTxt("dataset/2/neg/*.txt", 0)
dataset2pos <- getValuesFromTxt("dataset/2/pos/*.txt", 1)
data <- bind_rows(data, dataset2neg)
data <- bind_rows(data, dataset2pos)
## Randomize all data rows so there aren't humps of successive positive and negatives
data <- data[sample(nrow(data), nrow(data)), ]
data$doc_id <- seq.int(nrow(data))
## Save to Mongo
con=mongo(collection="reviews",db="movies")
con$drop()
con$insert(data)
con=mongo(collection="reviews",db="movies")
df <-con$find("{}")
# https://rpubs.com/cen0te/naivebayes-sentimentpolarity
# install.packages("mongolite")
# install.packages("tm")
# install.packages("RTextTools")
# install.packages("e1071")
# install.packages("dplyr")
# install.packages("caret")
# Load required libraries
library(tm)
library(RTextTools)
library(e1071)
library(dplyr)
library(caret)
library(mongolite)
con=mongo(collection="reviews",db="movies")
df <-con$find("{}")
set.seed(1)
df <- df[sample(nrow(df)), ]
df <- df[sample(nrow(df)), ]
df$sentiment <- as.factor(df$sentiment)
corpus <- Corpus(VectorSource(df$text))
corpus.clean <- corpus %>%
tm_map(content_transformer(tolower)) %>%
tm_map(removePunctuation) %>%
tm_map(removeNumbers) %>%
tm_map(removeWords, stopwords(kind="en")) %>%
tm_map(stripWhitespace)
dtm <- DocumentTermMatrix(corpus.clean)
df.train <- df[1:13240,]
df.test <- df[13241:15240,]
dtm.train <- dtm[1:13240,]
dtm.test <- dtm[13241:15240,]
corpus.clean.train <- corpus.clean[1:13240]
corpus.clean.test <- corpus.clean[13241:15240]
dim(dtm.train)
fivefreq <- findFreqTerms(dtm.train, 5)
length((fivefreq))
## [1] 12144
# Use only 5 most frequent words (fivefreq) to build the DTM
dtm.train.nb <- DocumentTermMatrix(corpus.clean.train, control=list(dictionary = fivefreq))
dim(dtm.train.nb)
## [1]  1500 12144
dtm.test.nb <- DocumentTermMatrix(corpus.clean.test, control=list(dictionary = fivefreq))
dim(dtm.train.nb)
convert_count <- function(x) {
y <- ifelse(x > 0, 1,0)
y <- factor(y, levels=c(0,1), labels=c("No", "Yes"))
y
}
trainNB <- apply(dtm.train.nb, 2, convert_count)
testNB <- apply(dtm.test.nb, 2, convert_count)
system.time( classifier <- naiveBayes(trainNB, df.train$sentiment, laplace = 1) )
system.time( pred <- predict(classifier, newdata=testNB) )
table("Predictions"= pred,  "Actual" = df.test$sentiment )
conf.mat <- confusionMatrix(pred, df.test$sentiment)
conf.mat
View(conf.mat)
View(conf.mat)
conf
conf.mat
View(df.test)
View(df.test)
dataset2neg <- getValuesFromTxt("dataset/2/neg/*.txt", 0)
dataset2pos <- getValuesFromTxt("dataset/2/pos/*.txt", 1)
data <- bind_rows(data, dataset2neg)
data <- bind_rows(data, dataset2pos)
## Randomize all data rows so there aren't humps of successive positive and negatives
data <- data[sample(nrow(data), nrow(data)), ]
data$doc_id <- seq.int(nrow(data))
## Save to Mongo
con=mongo(collection="reviews",db="movies")
con$drop()
con$insert(data)
data <- getValuesFromTxt("dataset/2/neg/*.txt", 0)
dataset2pos <- getValuesFromTxt("dataset/2/pos/*.txt", 1)
data <- bind_rows(data, dataset2neg)
data <- bind_rows(data, dataset2pos)
## Randomize all data rows so there aren't humps of successive positive and negatives
data <- data[sample(nrow(data), nrow(data)), ]
data$doc_id <- seq.int(nrow(data))
## Save to Mongo
con=mongo(collection="reviews",db="movies")
con$drop()
con$insert(data)
data <- getValuesFromTxt("dataset/2/neg/*.txt", 0)
dataset2pos <- getValuesFromTxt("dataset/2/pos/*.txt", 1)
data <- bind_rows(data, dataset2neg)
data <- bind_rows(data, dataset2pos)
data <- getValuesFromTxt("dataset/2/neg/*.txt", 0)
dataset2pos <- getValuesFromTxt("dataset/2/pos/*.txt", 1)
data <- getValuesFromTxt("dataset/2/neg/*.txt", 0)
getValuesFromTxt <- function(files, sentiment) {
ds <- readtext::readtext(files)
ds$doc_id <- NULL;
ds$sentiment <- sentiment;
colnames(ds)[1] <- "text"
ds
}
data <- getValuesFromTxt("dataset/2/neg/*.txt", 0)
data <- getValuesFromTxt("dataset/2/neg/*.txt", 0)
dataset2pos <- getValuesFromTxt("dataset/2/pos/*.txt", 1)
data <- bind_rows(data, dataset2neg)
data <- bind_rows(data, dataset2pos)
## Randomize all data rows so there aren't humps of successive positive and negatives
data <- data[sample(nrow(data), nrow(data)), ]
data$doc_id <- seq.int(nrow(data))
## Save to Mongo
con=mongo(collection="reviews",db="movies")
con$drop()
con$insert(data)
con=mongo(collection="reviews",db="movies")
df <-con$find("{}")
length(df)
count(df)
df_count <- count(df)
df_count <- nrow(df)
df_count
df_count /2
df_count / 3
ceiling(df_count / 3)
ceiling(df_count / 3) + 1
set.seed(1)
df <- df[sample(nrow(df)), ]
df <- df[sample(nrow(df)), ]
df$sentiment <- as.factor(df$sentiment)
corpus <- Corpus(VectorSource(df$text))
corpus.clean <- corpus %>%
tm_map(content_transformer(tolower)) %>%
tm_map(removePunctuation) %>%
tm_map(removeNumbers) %>%
tm_map(removeWords, stopwords(kind="en")) %>%
tm_map(stripWhitespace)
dtm <- DocumentTermMatrix(corpus.clean)
df.train <- df[1:ceiling(df_count / 2),]
df.test <- df[ceiling(df_count / 2) + 1:df_count,]
ceiling(df_count / 2) + 1:df_count
df_count
ceiling(df_count / 2) + 1
1001:2000
1:5
5:10
ceiling(10 / 2) + 1:10
ceiling(10 / 2) + 1:5
ceiling(10 / 2) + 1
df.test <- df[ceiling(df_count / 2) + 1:ceiling(df_count / 2),]
100:10
100:110
100+1:110
(100+1):110
dim(dtm.train)
df.train <- df[1:ceiling(df_count / 2),]
df.test <- df[(ceiling(df_count / 2) + 1):df_count,]
dtm.train <- dtm[1:ceiling(df_count / 2),]
dtm.test <- dtm[(ceiling(df_count / 2) + 1):df_count,]
corpus.clean.train <- corpus.clean[1:ceiling(df_count / 2)]
corpus.clean.test <- corpus.clean[(ceiling(df_count / 2) + 1):df_count]
dim(dtm.train)
View(df)
doc_id <- c(10000,10001,10002,10003,10004,10005,10006)
text <- c("very good", "very bad", "nice", "like", "hate")
df.test <- data.frame(doc_id, text)
doc_id <- c(10000,10001,10002,10003,10004,10005,10006)
text <- c("very good", "very bad", "nice", "like", "hate", "love")
df.test <- data.frame(doc_id, text)
doc_id <- c(10000,10001,10002,10003,10004,10005,10006)
text <- c("very good", "very bad", "nice", "like", "hate", "love", "great")
df.test <- data.frame(doc_id, text)
doc_id <- c(10000,10001,10002,10003,10004,10005,10006)
text <- c("very good", "very bad", "nice", "like", "hate", "love", "great")
df.test <- data.frame(doc_id, text)
test_corpus <- Corpus(VectorSource(df.test$text))
test_corpus.clean <- test_corpus %>%
tm_map(content_transformer(tolower)) %>%
tm_map(removePunctuation) %>%
tm_map(removeNumbers) %>%
tm_map(removeWords, stopwords(kind="en")) %>%
tm_map(stripWhitespace)
dtm.test <- DocumentTermMatrix(df.test)
corpus.clean.test <- test_corpus.clean
doc_id <- c(10000,10001,10002,10003,10004,10005,10006)
text <- c("very good", "very bad", "nice", "like", "hate", "love", "great")
df.test <- data.frame(doc_id, text)
test_corpus <- Corpus(VectorSource(df.test$text))
test_corpus.clean <- test_corpus %>%
tm_map(content_transformer(tolower)) %>%
tm_map(removePunctuation) %>%
tm_map(removeNumbers) %>%
tm_map(removeWords, stopwords(kind="en")) %>%
tm_map(stripWhitespace)
dtm.test <- DocumentTermMatrix(test_corpus.clean)
corpus.clean.test <- test_corpus.clean
fivefreq <- findFreqTerms(dtm.train, 5)
length((fivefreq))
## [1] 12144
# Use only 5 most frequent words (fivefreq) to build the DTM
dtm.train.nb <- DocumentTermMatrix(corpus.clean.train, control=list(dictionary = fivefreq))
dim(dtm.train.nb)
## [1]  1500 12144
dtm.test.nb <- DocumentTermMatrix(corpus.clean.test, control=list(dictionary = fivefreq))
dim(dtm.train.nb)
convert_count <- function(x) {
y <- ifelse(x > 0, 1,0)
y <- factor(y, levels=c(0,1), labels=c("No", "Yes"))
y
}
trainNB <- apply(dtm.train.nb, 2, convert_count)
testNB <- apply(dtm.test.nb, 2, convert_count)
system.time( classifier <- naiveBayes(trainNB, df.train$sentiment, laplace = 1) )
system.time( pred <- predict(classifier, newdata=testNB) )
table("Predictions"= pred,  "Actual" = df.test$sentiment )
conf.mat <- confusionMatrix(pred, df.test$sentiment)
conf.mat
doc_id <- c(10000,10001,10002,10003,10004,10005,10006)
text <- c("very good", "very bad", "nice", "like", "hate", "love", "great")
sentiment <- c(1, 0, 1, 1, 0, 1, 1)
df.test <- data.frame(doc_id, text, sentiment)
test_corpus <- Corpus(VectorSource(df.test$text))
test_corpus.clean <- test_corpus %>%
tm_map(content_transformer(tolower)) %>%
tm_map(removePunctuation) %>%
tm_map(removeNumbers) %>%
tm_map(removeWords, stopwords(kind="en")) %>%
tm_map(stripWhitespace)
dtm.test <- DocumentTermMatrix(test_corpus.clean)
corpus.clean.test <- test_corpus.clean
fivefreq <- findFreqTerms(dtm.train, 5)
length((fivefreq))
## [1] 12144
# Use only 5 most frequent words (fivefreq) to build the DTM
dtm.train.nb <- DocumentTermMatrix(corpus.clean.train, control=list(dictionary = fivefreq))
dim(dtm.train.nb)
## [1]  1500 12144
dtm.test.nb <- DocumentTermMatrix(corpus.clean.test, control=list(dictionary = fivefreq))
dim(dtm.train.nb)
convert_count <- function(x) {
y <- ifelse(x > 0, 1,0)
y <- factor(y, levels=c(0,1), labels=c("No", "Yes"))
y
}
trainNB <- apply(dtm.train.nb, 2, convert_count)
testNB <- apply(dtm.test.nb, 2, convert_count)
system.time( classifier <- naiveBayes(trainNB, df.train$sentiment, laplace = 1) )
system.time( pred <- predict(classifier, newdata=testNB) )
table("Predictions"= pred,  "Actual" = df.test$sentiment )
conf.mat <- confusionMatrix(pred, df.test$sentiment)
conf.mat
View(dtm.test)
View(df.test)
conf.mat <- confusionMatrix(pred, df.test$sentiment)
View(df.train)
dtm.test.nb;
df.test$sentiment
pred
typeof(df.test$sentiment)
typeof(pred)
typeof(sentiment)
sentiment <- c(1, 0, 1, 1, 0, 1, 1)
as.numeric(sentiment)
sentiment <- c(1, 0, 1, 1, 0, 1, 1)
as.numeric(sentiment)
typeof(sentiment)
sentiment <- as.numeric(sentiment)
sentiment <- as.numeric(sentiment)
typeof(sentiment)
bla <- c(1,2)
typeof(bla)
typeof(df.test$sentiment)
as.int(df.test$sentiment)
as.integer(df.test$sentiment)
as.int(df.test$sentiment)
typeof(df.test$sentiment)
typeof(pred)
as.integer(df.test$sentiment)
typeof(df.test$sentiment)
sentiment <- c(1, 0, 1, 1, 0, 1, 1)
sentiment <- as.integer(sentiment)
typeof(sentiment)
## Custom test set
doc_id <- c(10000,10001,10002,10003,10004,10005,10006)
text <- c("very good", "very bad", "nice", "like", "hate", "love", "great")
sentiment <- c(1, 0, 1, 1, 0, 1, 1)
sentiment <- as.integer(sentiment)
df.test <- data.frame(doc_id, text, sentiment)
test_corpus <- Corpus(VectorSource(df.test$text))
test_corpus.clean <- test_corpus %>%
tm_map(content_transformer(tolower)) %>%
tm_map(removePunctuation) %>%
tm_map(removeNumbers) %>%
tm_map(removeWords, stopwords(kind="en")) %>%
tm_map(stripWhitespace)
dtm.test <- DocumentTermMatrix(test_corpus.clean)
corpus.clean.test <- test_corpus.clean
fivefreq <- findFreqTerms(dtm.train, 5)
length((fivefreq))
## [1] 12144
# Use only 5 most frequent words (fivefreq) to build the DTM
dtm.train.nb <- DocumentTermMatrix(corpus.clean.train, control=list(dictionary = fivefreq))
dim(dtm.train.nb)
## [1]  1500 12144
dtm.test.nb <- DocumentTermMatrix(corpus.clean.test, control=list(dictionary = fivefreq))
dim(dtm.train.nb)
convert_count <- function(x) {
y <- ifelse(x > 0, 1,0)
y <- factor(y, levels=c(0,1), labels=c("No", "Yes"))
y
}
trainNB <- apply(dtm.train.nb, 2, convert_count)
testNB <- apply(dtm.test.nb, 2, convert_count)
system.time( classifier <- naiveBayes(trainNB, df.train$sentiment, laplace = 1) )
system.time( pred <- predict(classifier, newdata=testNB) )
table("Predictions"= pred,  "Actual" = df.test$sentiment )
conf.mat <- confusionMatrix(pred, df.test$sentiment)
conf.mat
typeof(sentiment)
typeof(df.test$sentiment)
typeof(pred)
table("Predictions"= pred,  "Actual" = df.test$sentiment )
conf.mat <- confusionMatrix(pred, df.test$sentiment)
pred
confusionMatrix()
system.time( classifier <- naiveBayes(trainNB, df.train$sentiment, laplace = 1) )
system.time( pred <- predict(classifier, newdata=testNB) )
pred
conf.mat <- confusionMatrix(pred, df.test$sentiment)
table("Predictions"= pred,  "Actual" = df.test$sentiment )
typeof(pred)
typeof(df.test$sentiment)
con=mongo(collection="reviews",db="movies")
df <-con$find("{}")
df_count <- nrow(df)
set.seed(1)
df <- df[sample(nrow(df)), ]
df <- df[sample(nrow(df)), ]
df$sentiment <- as.factor(df$sentiment)
corpus <- Corpus(VectorSource(df$text))
corpus.clean <- corpus %>%
tm_map(content_transformer(tolower)) %>%
tm_map(removePunctuation) %>%
tm_map(removeNumbers) %>%
tm_map(removeWords, stopwords(kind="en")) %>%
tm_map(stripWhitespace)
dtm <- DocumentTermMatrix(corpus.clean)
df.train <- df[1:ceiling(df_count / 2),]
df.test <- df[(ceiling(df_count / 2) + 1):df_count,]
dtm.train <- dtm[1:ceiling(df_count / 2),]
dtm.test <- dtm[(ceiling(df_count / 2) + 1):df_count,]
corpus.clean.train <- corpus.clean[1:ceiling(df_count / 2)]
corpus.clean.test <- corpus.clean[(ceiling(df_count / 2) + 1):df_count]
dim(dtm.train)
## Custom test set
doc_id <- c(10000,10001,10002,10003,10004,10005,10006)
text <- c("very good", "very bad", "nice", "like", "hate", "love", "great")
sentiment <- c(1, 0, 1, 1, 0, 1, 1)
sentiment <- as.integer(sentiment)
df.test <- data.frame(doc_id, text, sentiment)
test_corpus <- Corpus(VectorSource(df.test$text))
test_corpus.clean <- test_corpus %>%
tm_map(content_transformer(tolower)) %>%
tm_map(removePunctuation) %>%
tm_map(removeNumbers) %>%
tm_map(removeWords, stopwords(kind="en")) %>%
tm_map(stripWhitespace)
dtm.test <- DocumentTermMatrix(test_corpus.clean)
corpus.clean.test <- test_corpus.clean
fivefreq <- findFreqTerms(dtm.train, 5)
length((fivefreq))
## [1] 12144
# Use only 5 most frequent words (fivefreq) to build the DTM
dtm.train.nb <- DocumentTermMatrix(corpus.clean.train, control=list(dictionary = fivefreq))
dim(dtm.train.nb)
## [1]  1500 12144
dtm.test.nb <- DocumentTermMatrix(corpus.clean.test, control=list(dictionary = fivefreq))
dim(dtm.train.nb)
convert_count <- function(x) {
y <- ifelse(x > 0, 1,0)
y <- factor(y, levels=c(0,1), labels=c("No", "Yes"))
y
}
trainNB <- apply(dtm.train.nb, 2, convert_count)
testNB <- apply(dtm.test.nb, 2, convert_count)
system.time( classifier <- naiveBayes(trainNB, df.train$sentiment, laplace = 1) )
system.time( pred <- predict(classifier, newdata=testNB) )
table("Predictions"= pred,  "Actual" = df.test$sentiment )
conf.mat <- confusionMatrix(pred, df.test$sentiment)
conf.mat
## Custom test set
doc_id <- c(10000,10001,10002,10003,10004,10005,10006)
text <- c("very good", "very bad", "nice", "like", "hate", "love", "I really like this movie. Awesome")
sentiment <- c(1, 0, 1, 1, 0, 1, 1)
sentiment <- as.integer(sentiment)
df.test <- data.frame(doc_id, text, sentiment)
test_corpus <- Corpus(VectorSource(df.test$text))
test_corpus.clean <- test_corpus %>%
tm_map(content_transformer(tolower)) %>%
tm_map(removePunctuation) %>%
tm_map(removeNumbers) %>%
tm_map(removeWords, stopwords(kind="en")) %>%
tm_map(stripWhitespace)
dtm.test <- DocumentTermMatrix(test_corpus.clean)
corpus.clean.test <- test_corpus.clean
fivefreq <- findFreqTerms(dtm.train, 5)
length((fivefreq))
## [1] 12144
# Use only 5 most frequent words (fivefreq) to build the DTM
dtm.train.nb <- DocumentTermMatrix(corpus.clean.train, control=list(dictionary = fivefreq))
dim(dtm.train.nb)
## [1]  1500 12144
dtm.test.nb <- DocumentTermMatrix(corpus.clean.test, control=list(dictionary = fivefreq))
dim(dtm.train.nb)
convert_count <- function(x) {
y <- ifelse(x > 0, 1,0)
y <- factor(y, levels=c(0,1), labels=c("No", "Yes"))
y
}
trainNB <- apply(dtm.train.nb, 2, convert_count)
testNB <- apply(dtm.test.nb, 2, convert_count)
system.time( classifier <- naiveBayes(trainNB, df.train$sentiment, laplace = 1) )
system.time( pred <- predict(classifier, newdata=testNB) )
table("Predictions"= pred,  "Actual" = df.test$sentiment )
conf.mat <- confusionMatrix(pred, df.test$sentiment)
conf.mat
View(dtm.train.nb)
View(dtm.train.nb)
View(dtm.test.nb)
corpus.clean.test <- test_corpus.clean[,]
corpus.clean.test <- test_corpus.clean[]
typeof(corpus.clean.train)
typeof(corpus.clean.test)
View(df.train)
View(df.test)
system.time( pred <- predict(classifier, newdata=testNB,na.action = na.pass) )
table("Predictions"= pred,  "Actual" = df.test$sentiment )
conf.mat <- confusionMatrix(pred, df.test$sentiment)
